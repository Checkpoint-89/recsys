{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "browsing_path = r\"D:\\Data\\Documents\\recsys\\SIGIR-ecom-data-challenge\\train\\browsing_train.csv\"\n",
    "data = pd.read_csv(browsing_path)\n",
    "\n",
    "# Add event_product column (type of event + type of product)\n",
    "data[['event_type', 'product_action']] = data[['event_type', 'product_action']].fillna(\"NaN\")\n",
    "data['event_product'] = data['event_type'] + '/' + data['product_action']\n",
    "\n",
    "# Compute what are the combinations of events and products\n",
    "event_action = data['event_product'].value_counts(dropna=False)\n",
    "uniques = data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (36079307, 7)\n",
      "\n",
      "Data columns:\n",
      " ['session_id_hash', 'event_type', 'product_action', 'product_sku_hash', 'server_timestamp_epoch_ms', 'hashed_url', 'event_product']\n",
      "\n",
      "Uniques:\n",
      " session_id_hash               4934699\n",
      "event_type                          2\n",
      "product_action                      5\n",
      "product_sku_hash                57483\n",
      "server_timestamp_epoch_ms    26219566\n",
      "hashed_url                     489300\n",
      "event_product                       5\n",
      "dtype: int64\n",
      "\n",
      "Event_product:\n",
      " pageview/NaN              25647696\n",
      "event_product/detail       9707890\n",
      "event_product/add           329557\n",
      "event_product/remove        316316\n",
      "event_product/purchase       77848\n",
      "Name: event_product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display some info about the dataset\n",
    "print(\"Data shape: \", data.shape)\n",
    "print(\"\\nData columns:\\n\", data.columns.to_list())\n",
    "print(\"\\nUniques:\\n\", uniques)\n",
    "print(\"\\nEvent_product:\\n\", event_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sessions  = 0 product:  1958881.0\n",
      "# sessions  = 1 product:  553270.0\n",
      "# sessions >= 2 products:  768433.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keep only session and product ids columns\n",
    "# Drop lines that dont concern products \n",
    "data_ = data[['session_id_hash', 'product_sku_hash']].dropna(subset='product_sku_hash')\n",
    "products_in_session = data_.groupby(by='session_id_hash')\n",
    "hist = pyplot.hist(products_in_session.nunique().to_numpy(), bins=100)\n",
    "print(\"# sessions  = 1 product: \", hist[0][0])\n",
    "print(\"# sessions  = 2 product: \", hist[0][1])\n",
    "print(\"# sessions >= 3 products: \", hist[0][2:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 10431611\n",
      "2: 109923643\n",
      "3: 38963186\n",
      "4: 31706918\n",
      "5: 31706918\n",
      "6: 15853459\n",
      "7: 15853459\n"
     ]
    }
   ],
   "source": [
    "del(data)\n",
    "print(\"1:\", len(data_))\n",
    "\n",
    "# Compute the the pairs of 'products' in a same 'session'\n",
    "data_ = pd.merge(data_, data_, on='session_id_hash', suffixes=(\"_1\", \"_2\"))\n",
    "print(\"2:\", len(data_))\n",
    "\n",
    "# Drop duplicates of the same pair of products (A B on several rows)\n",
    "data_ = data_.drop_duplicates()\n",
    "print(\"3:\", len(data_))\n",
    "\n",
    "# Drop pairs of the same product (A A on a row)\n",
    "data_ = data_[data_['product_sku_hash_1']!=data_['product_sku_hash_2']]\n",
    "print(\"4:\", len(data_))\n",
    "\n",
    "# Transform the DataFrame into a list\n",
    "data_ = data_[['session_id_hash', 'product_sku_hash_1', 'product_sku_hash_2']].values.tolist()\n",
    "print(\"5:\", len(data_))\n",
    "\n",
    "# Transform the list into a set to discard pairs of same products (A B and B A)\n",
    "data_ = {frozenset(l) for l in data_}\n",
    "print(\"6:\", len(data_))\n",
    "\n",
    "# Transform into a DataFrame and save to a csv\n",
    "data_=pd.DataFrame(data_)\n",
    "data_.columns = [['session_id_hash', 'product_sku_hash_1', 'product_sku_hash_2']]\n",
    "print(\"7:\", len(data_))\n",
    "data_.to_csv(\"pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pyplot.hist(products_in_session.nunique().to_numpy(), bins=100)\n",
    "print(\"# sessions  = 0 product: \", hist[0][0])\n",
    "print(\"# sessions  = 1 product: \", hist[0][1])\n",
    "print(\"# sessions >= 2 products: \", hist[0][2:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_in_session = products_in_session.dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for name, group in products_in_session:\n",
    "    print(\"Name: \", name)\n",
    "    print(\"Group:\\n\", combinations(group.unique(),2))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(combinations(group.unique(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"session_id_hash\"]=='00000114e1075962f022114fcfc17f2d874e694ac5d2010985bbba0a595340db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = [[\n",
    "    \"Salut\", \"comment\", \"ca\", \"va\", \"?\",\n",
    "]]\n",
    "\n",
    "output_embeddings = [[\n",
    "    \"<START>\", \"Hi\", \"how\", \"are\", \"you\", \"?\",\n",
    "]]\n",
    "print(input_embeddings)\n",
    "print(output_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(sequences):\n",
    "\n",
    "    token_to_info = {}\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for word in sequence:\n",
    "            if word not in token_to_info:\n",
    "                token_to_info[word] = len(token_to_info)\n",
    "    return token_to_info\n",
    "\n",
    "input_voc = get_vocabulary(input_embeddings)\n",
    "output_voc = get_vocabulary(output_embeddings)\n",
    "\n",
    "input_voc[\"<START>\"] = len(input_voc)\n",
    "input_voc[\"<END>\"] = len(input_voc)\n",
    "input_voc[\"<PAD>\"] = len(input_voc)\n",
    "\n",
    "output_voc[\"<END>\"] = len(output_voc)\n",
    "output_voc[\"<PAD>\"] = len(output_voc)\n",
    "\n",
    "print(input_voc)\n",
    "print(output_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_to_int(sequences, voc):\n",
    "    for sequence in sequences:\n",
    "        for s, word in enumerate(sequence):\n",
    "            sequence[s] = voc[word]\n",
    "    return(np.array(sequences))\n",
    "\n",
    "input_seq = sequences_to_int(input_embeddings, input_voc)\n",
    "output_seq = sequences_to_int(output_embeddings, output_voc)\n",
    "\n",
    "print(input_seq)\n",
    "print(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_token, **kwargs):\n",
    "        self.nb_token = nb_token\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.word_embedding = tf.keras.layers.Embedding(\n",
    "            self.nb_token, 256,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        embed = self.word_embedding(x)\n",
    "        return embed\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK / tf.math.sqrt(256.)\n",
    "        softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
    "        attention = tf.matmul(softmax_QK, V)\n",
    "        # print(\"Shape Q\", Q.shape)\n",
    "        # print(\"Shape K\", K.shape)\n",
    "        # print(\"Shape V\", V.shape)\n",
    "        # print(\"Shape QK\", QK.shape)\n",
    "        # print(\"Shape softmax\", softmax_QK.shape)\n",
    "        # print(\"Shape attention\", attention.shape)\n",
    "        return attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    attention = ScaledDotProductAttention()(embedding)\n",
    "    model = tf.keras.Model(layer_input, attention)\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, dim=256, nb_head=8, **kwargs):\n",
    "        self.dim = 256\n",
    "        self.head_dim = 256 // 8\n",
    "        self.nb_head = nb_head\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query_layer = tf.keras.layers.Dense(256)\n",
    "        self.value_layer = tf.keras.layers.Dense(256)\n",
    "        self.key_layer = tf.keras.layers.Dense(256)\n",
    "        self.out_proj = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def mask_softmax(self, x, mask):\n",
    "        x_expe = tf.math.exp(x)\n",
    "        x_expe_masked = x_expe * mask\n",
    "        x_expe_sum = tf.reduce_sum(x_expe_masked, axis = -1)\n",
    "        x_expe_sum = tf.expand_dims(x_expe_sum, axis=-1)\n",
    "        softmax = x_expe_masked / x_expe_sum\n",
    "        return softmax\n",
    "\n",
    "    def call(self, x, mask = None):\n",
    "\n",
    "        in_query, in_key, in_value = x\n",
    "\n",
    "        Q = self.query_layer(in_query)\n",
    "        K = self.key_layer(in_key)\n",
    "        V = self.value_layer(in_value)\n",
    "\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_seq_len = tf.shape(Q)[1]\n",
    "        K_seq_len = tf.shape(K)[1]\n",
    "        V_seq_len = tf.shape(V)[1]\n",
    "\n",
    "        Q = tf.reshape(Q, [batch_size, Q_seq_len, self.nb_head, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size, K_seq_len, self.nb_head, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size, V_seq_len, self.nb_head, self.head_dim])\n",
    "\n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "\n",
    "        Q = tf.reshape(Q, [batch_size * self.nb_head, Q_seq_len, self.head_dim])\n",
    "        K = tf.reshape(K, [batch_size * self.nb_head, K_seq_len, self.head_dim])\n",
    "        V = tf.reshape(V, [batch_size * self.nb_head, V_seq_len, self.head_dim])\n",
    "\n",
    "        # Scaled dot product attention\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        QK = QK / tf.math.sqrt(256.)\n",
    "\n",
    "        if mask is not None:\n",
    "            QK = QK * mask\n",
    "            softmax_QK = self.mask_softmax(QK, mask)\n",
    "        else:\n",
    "            softmax_QK = tf.nn.softmax(QK, axis=-1)\n",
    "\n",
    "        attention = tf.matmul(softmax_QK, V)\n",
    "        attention = tf.reshape(attention, [batch_size, self.nb_head, Q_seq_len, self.head_dim])\n",
    "        attention = tf.transpose(attention, [0, 2, 1, 3])\n",
    "\n",
    "        # Concat\n",
    "        attention = tf.reshape(attention, [batch_size, Q_seq_len, self.nb_head * self.head_dim])\n",
    "\n",
    "        out_attention = self.out_proj(attention)\n",
    "\n",
    "        return out_attention\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(6))\n",
    "    embedding = EmbeddingLayer(nb_token=6)(layer_input)\n",
    "\n",
    "    mask = tf.sequence_mask(tf.range(6) + 1, 6)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=0)\n",
    "    multi_attention = MultiHeadAttention()((embedding,embedding, embedding), mask =mask)\n",
    "\n",
    "    model = tf.keras.Model(layer_input, multi_attention)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(output_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.multi_head_attention = MultiHeadAttention()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dense_out = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        attention = self.multi_head_attention((x, x, x))\n",
    "        post_attention = self.norm(x + attention)\n",
    "        x = self.dense_out(post_attention)\n",
    "        enc_output = self.norm(x + post_attention)\n",
    "        return enc_output\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_encoder, **kwargs):\n",
    "        self.nb_encoder = nb_encoder\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.encoder_layers = []\n",
    "        for nb in range(self.nb_encoder):\n",
    "            self.encoder_layers.append(\n",
    "                EncoderLayer()\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "def test():\n",
    "    layer_input = tf.keras.Input(shape=(5))\n",
    "    embedding = EmbeddingLayer(nb_token=5)(layer_input)\n",
    "    enc_output = Encoder(nb_encoder=6)(embedding)\n",
    "    model = tf.keras.Model(layer_input, enc_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test(input_seq)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.multi_head_self_attention = MultiHeadAttention()\n",
    "        self.multi_head_enc_attention = MultiHeadAttention()\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "        self.proj_output = tf.keras.layers.Dense(256)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        enc_output, output_embedding, mask = x\n",
    "        self_attention = self.multi_head_self_attention((output_embedding, output_embedding, output_embedding), mask)\n",
    "        post_self_att = self.norm(output_embedding + self_attention)\n",
    "        enc_attention = self.multi_head_enc_attention((post_self_att, enc_output, enc_output)) # Pas sur de l'ordre\n",
    "        post_enc_attention = self.norm(enc_attention + post_self_att)\n",
    "        proj_out = self.proj_output(post_enc_attention)\n",
    "        dec_output = self.norm(proj_out + post_enc_attention)\n",
    "        return dec_output\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, nb_decoder, **kwargs):\n",
    "        self.nb_decoder = nb_decoder\n",
    "        super(**kwargs).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.decoder_layers = []\n",
    "        for nb in range(self.nb_decoder):\n",
    "            self.decoder_layers.append(\n",
    "                DecoderLayer()\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "\n",
    "        enc_out, output_embedding, mask = x\n",
    "        dec_output = output_embedding\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            dec_output = decoder_layer((enc_out, dec_output, mask))\n",
    "        return dec_output\n",
    "\n",
    "def test():\n",
    "    input_token = tf.keras.Input(shape=(5))\n",
    "    output_token = tf.keras.Input(shape=(6))\n",
    "\n",
    "    # Retrieve embedding\n",
    "    input_embedding = EmbeddingLayer(nb_token=5)(input_token)\n",
    "    output_embedding = EmbeddingLayer(nb_token=6)(output_token)\n",
    "\n",
    "    # Encoder\n",
    "    enc_output = Encoder(nb_encoder=6)(input_embedding)\n",
    "\n",
    "    # mask\n",
    "    mask = tf.sequence_mask(tf.range(6) + 1, 6)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=0)\n",
    "\n",
    "    # Decoder\n",
    "    dec_output = Decoder(nb_decoder=6)((enc_output, output_embedding, mask))\n",
    "\n",
    "    model = tf.keras.Model([input_token, output_token], dec_output)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "m_test = test()\n",
    "out = m_test((input_seq, output_seq))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('recsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d34aa7b8271b882c4eaa14b3a9c79628959a28c28a8d4df4e5ffabe9667f584"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
